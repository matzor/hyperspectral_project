\section{Dimensionality Reduction \& Noise Filtering}
\label{sec:dim_reduction}

The HICO image cube is converted to a matrix on the form L x N, 
with $L$ spectral bands rows, and $N = WH$ columns. 

\subsection{What is dimensionality reduction?}

Dimensionality reduction is the act of reducing the amount of random variables 
to consider in for instance machine learning and statistics, involving feature 
selection and feature extraction. Thus making the data smaller and making 
analyzing it faster and easier. 

Considering \cref{fig:point_spectra}, we see that there are quite a few similarities 
in the spectra of the various points. Looking at the two points in the water (shallow 
and deep), we see that the spectra are almost identical to each other for wavelengths 
between around 400 - 470nm, except for the clear difference in the amplitude. Even the 
spectra for the vegetation seem to share this characteristic. For wavelengths between 
470 - 800nm all spectra seem to have unique signatures, until the two points in the 
water again seem to share quite similar spectra for wavelengths over 800nm, with only 
small variations between the two. An other thing to consider is the fact that the 
atmospheric correction algorithm used in \cref{sec:atmoshperic_cor} is invalid for 
wavelengths outside the range 438 - 730nm, so these invalid bands can be dropped. 
Thus there seems to be some opportunities to reduce the dimensionality in the 
spectral dimensions for the aforementioned wavelengths.

In the spatial direction, we could for instance ignore all of the landmass, as we 
are only interested in looking at the water. This can be done as described in 
\cref{sec:landmask}, where all points on land are simply set to 0, thus reducing the 
spatial dimension significantly by removing a big chunk of the image. The spatial 
dimension can be reduced further by doing a nearest neighbor approach to cluster data 
together, for instance in the deep water in the upper left part of the image, \cref{fig:pseudo_rgb}, 
where a portion of the ocean seem to be very similar in appearance. This can be done 
using for example k-means clustering, as described previously in \cref{sec:classify}. 

\subsection{Principal Component Analysis (PCA)}

PCA is a dimensionality reduction technique that transforms the columns of a dataset 
into a new set features, by finding a new set of directions that explain the maximum 
variability in the data \cite{prabhakaran2019}. These new coordinate axes/directions 
are known as the Principal Components (PCs). The dataset columns contains the amount 
of variance of the data, computing the PCs will help explain the vast information in 
the original data in a fewer amount of columns. 

If we do PCA on a matrix $X$ with 100 rows and 1000 columns, using 10 basis vectors 
the PCA components will have the following shapes:

\begin{itemize}
    \item Original data shape:  (100, 1000)
    \item PCA dataframe shape:  (10, 1000)
    \item PCA weights shape:  (100, 10)
\end{itemize}

Meaning we have 10 weights per column, $10 \times 1000 = 10000$ weights in total. 

The weights in the PCA is the eigenvectors of X. Each principal component is the dot 
product of its weights and the mean-centered data (each column of X is subtracted from 
its own mean, so the mean of each column is zero). 

\begin{equation}
    PC_i = weights \cdot X_{meancentered}
\end{equation}


\subsection{How does dimensionality reduction via PCA afect classification?}

PCA was implemented using the existing PCA algorithm from \textit{sklearn.decomposition}. 
There was also an attempt to implement PCA from scratch, this implementation can be found in 
the attached file \textit{pca.py}, but this gave reduced performance compared to the sklearn 
function, so the latter was used for the rest of the report. 
First we convert the HICO image cube into matrix form, as described in \cref{sec:dim_reduction}, 
We then run the PCA with $P = 10$ principal components on this data matrix, returning 
an $P \times N$ size matrix representing the compressed data. That is $10 \times 250000$ in size, 
down from $100 \times 250000$, a ten times reduction in size. Looking at the variance 
of the 10 principle components, shown in \cref{eq:pca_var} and plotted in \cref{fig:pca_var}, we 
clearly see that all the variance in the dataset is contained in the first 3 principle components, 
with a total of 91\% being in the first component alone. 
Thus we could potentially reduced the dataset even further, down to only 3 components, without 
losing any significant data. 

\begin{equation}
    \label{eq:pca_var}
    [0.91, 0.08, 0.01, 0,   0,   0,   0,   0,   0,   0,  ]
\end{equation}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{../fig/pca/variance_cumsum.png}
    \caption{Plotting the cumulative variance of the principal components}
    \label{fig:pca_var}
\end{figure}

Running K-means clustering on the PCA compressed data we notice a great increase in performance. 
Clustering the compressed data, running kmeans for 1-10 classes and max 50 iterations, 
now only takes about 53 seconds, compared to the 128 seconds 
it takes to classify the uncompressed data. The resulting clustering can be seen in 
\cref{fig:pca_kmean}, which looks more or less identical to the clustering from the 
original data in \cref{fig:kmean}. From the cluster means plot (right on \cref{fig:pca_kmean}) 
we see that there is almost no variation for $PC > 2$, except for the one outlier.

Reducing the number of PCs to $P = 3$ we get even faster performance, using only 
22 seconds to do the classification. The resulting image is identical to \cref{fig:pca_kmean}, 
as all the principal component containing all the variance/information in the dataset is still kept, 
even though the data is significantly more compressed. 

If the PCA is done using 100 principle components instead, we get a negative impact on the 
performance. Now the same kmeans clustering spends 145 seconds, while giving the same 
results as with 10 or even 3 components.

Turning P even lower, to $P = 1$, the kmeans finishes after only 15 seconds. However, we 
have lost some information. It is no longer possible to classify the data correctly as we 
have removed too much information. Some noticeable differences here is the inability to 
separate the different classes of the ocean, as there is no longer individual classes for 
shallow and deep water, but just one class in total for the ocean. Thus, this data is now 
useless for our purposes. 


\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{../fig/kmean/PCA_8}
    \caption{K-means classification performed on the PCA compressed data, using 10 Principal Components.}
    \label{fig:pca_kmean}
\end{figure}


\subsection{Maximum Noise Fraction}
\label{sec:mnf}

Maximum Noise Fraction, as described in \cite{green1988}, is summarized here and in \cite{assignment}. 
It is also known as Noise Adjusted Principal Components, and will here be used to 
remove additive noise on the image. 


\begin{equation}
    Cov(X) = \Sigma = \Sigma_s + \Sigma_n
\end{equation}

$\Sigma_s$ is the covariance of the signal and $\Sigma_n$ the covariance of the noise. 
We find the eigendecomposition of $\Sigma_n \Sigma^{-1}$ to find the left-hand eigenvectors 
$A^T$ and the right hand eigenvectors $V$. We then use these to calculate $\hat{X}$ for \cref{eq:mnf}.

\begin{equation}
    Y = A^T X
\end{equation}

\begin{equation}
    \label{eq:mnf}
    \hat{X} = V_{(:, 1:P)} Y_{(1:P, :)}
\end{equation}

The MNF algorithm is tested on 2 different test cases, with 1 and 2 components. The result from case 1 with 
1 component is shown in \cref{fig:mnf_test_1_1}, and result from case 2 with 2 components in \cref{fig:mnf_test_2_2}. 

For the first case, we find that using only one component gives the best result, for both PCA and MNF. Using 
2 components, we get an average pixel error of 47.4\% for MNF and 47.4\% for PCA, whilst with only one component, 
the error becomes 37.2\% and 37.2\% for MNF and PCA both. This implies that parts of the variance of 
the noise may be contained into the principle component that gets dropped when only keeping one component. 
The overall results are very similar, there is no clear difference between the MNF and PCA results. For comparison, 
the mean error of the unfiltered image is 59.6\%, so it is clear that both algorithms improves the error. 

For case 2 however, we find that using 1 component works better for PCA, but 2 components 
work better for MNF. Using 1 component we get 44.1\% for MNF and 40.0\% for PCA, but 
when using $P = 2$ we get an error of only 4.9\% for MNF while PCA has 46.3\% error. 
In this case, the MNF manages to remove almost all of the noise from the image, giving a result 
that looks almost identical to the original image.


\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{../fig/task_3_case_1_P_1}
    \caption{Maximum Noise Fraction and Principal Components analysis performed on test case 1, with 1 component used}
    \label{fig:mnf_test_1_1}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{../fig/task_3_case_2_P_2}
    \caption{Maximum Noise Fraction and Principal Components analysis performed on test case 2, with 2 component used. 
    Between-neighbor difference is used to approximate the noise covariance used in the MNF.}
    \label{fig:mnf_test_2_2}
\end{figure}


\subsection{Maximum Noise Fraction on HICO noisy}

To do MNF on the HICO\_noisy dataset, we first have to estimate the noise component. This is 
done using the between-neighbor difference, assuming neighboring pixels have similar 
spectra and thus the difference between the neighbors is the noise. 

\begin{equation}
    X_n = [x_1 - x_2, x_2 - x_3, \dots, x_{N-1} - x_N] = [\hat{n}_1, \hat{n}_2, \dots, \hat{n}_{N-1}]
\end{equation}

After estimating the noise component, we find the covariance $\Sigma_n$ of it. The \textit{HICO\_noisy} 
image cube is then ran through the MNF algorithm using various different amount of components. PCA 
is also performed, using the same number of components. The result can be found in \cref{table:mnf_pca}

\begin{table}[h]
    \centering
    \begin{tabular}{c|ccccccc}
        \textbf{P}   & \textbf{100} & \textbf{50} & \textbf{30} & \textbf{10} & \textbf{5} & \textbf{3} & \textbf{1} \\ \hline
        \textbf{MNF} & 0.1023       & 0.0724      & 0.0561      & 0.0325      & 0.0274     & 0.0312     & 0.2424     \\
        \textbf{PCA} & 0.1023       & 0.0729      & 0.0566      & 0.0330      & 0.0254     & 0.0268     & 0.1048    
    \end{tabular} 
    \caption{Comparing the resulting average pixel error of using MNF and PCA on the HICO\_noisy dataset
    using different number of Principal Components (P).}
    \label{table:mnf_pca}
\end{table}

As we can see from the table, the difference between MNF and PCA is quite small. They perform very similarly 
for most $P$s, but they have some differences for smaller $P$. Using $P = 1$ we find the largest difference, 
where PCA show an error of 10.48\% while MNF show an error of 24.24\%. Both PCA and MNF have the smallest error 
for $P = 5$, where they get 2.54\% and 2.74\% respectively. 

The RGB representation of the HICO\_noisy data can be seen in \cref{fig:HICO_mnf}. Here the 
MNF has been performed using 5 components, which gave the smallest error. As can be seen, 
the noise has been significantly reduced. 

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{../fig/HICO_mnf.png}
    \caption{Removing the noise from the noisy HICO image (left) using Maximum Noise Fraction (right)}
    \label{fig:HICO_mnf}
\end{figure}


\subsection{Discussing the results}

An assumption of PCA is that we have a reasonably high signal to noise ratio. Thus, 
if that assumption is true, PCA would be optimal. If the signal to noise ratio is low 
however, MNF would perform better. Also, looking at the results on the test image 
case 2 in \cref{sec:mnf}, \cref{fig:mnf_test_2_2}, the noise in the image seems to be only one single color, 
all the noise seems to be blue, compared to case 1 where the noise is random colors. 
In this case (case 2), MNF outperforms PCA significantly. But then again, the signal 
to noise ratio for case 2 is also lower than it is for case 1, so the result might 
very well be due to that fact, which is supported by the assumption given earlier. 

The Maximum Noise Fraction (MNF) is similar to the Principal Components Analysis (PCA), 
with the main difference being that the Principal Components associated with the MNF are 
ordered by descending signal-to-noise ratio rather than overall image variance. 

\subsection{How can we best use the subspace?}

Dimensionality reduction and compression are very useful tools when working with 
large datasets like hyperspectral images as discussed in the previous sections, 
but there are some disadvantages to it. An important thing to consider with PCA is 
that it is a lossy compression technique. 
When we perform the PCA, we lose some data and we are noe able to recreate this lost 
data completely. Many encoding based compression techniques on the other hand can be 
lossless. Another thing is that PCA might fail to capture all the discriminant 
information of hyperspectral images, since features that are important for classification 
tasks may not be high in signal energy. \cite{hybrid_compression}

Apart from the fact that dimensionality reductions like PCA makes the dataset 
smaller and easier to work with, it may also help in identifying underlying 
information in the data. Looking back to the chlorophyll identification in 
\cref{sec:task2}, we find that the chlorophyll may be found in the third 
principal component found by the PCA algorithm. In \cref{fig:pca_obpg}, we 
first do PCA with 2 and 3 principal components on the HICO dataset, before 
correcting for the atmospheric absorption and running the NASA OBPG algorithm 
to find the chlorophyll content of the ocean. Comparing the two cases, we 
find that the first case, using P = 2, does not show any sign of the algae, 
but we still see a difference between deep and shallow water. In the latter 
case, with P = 3, we again see a image with similar characteristic to the one 
in \cref{fig:mask_obpg} from the original, uncompressed data. Thus the 
chlorophyll seems to be isolated in principal component number 3, and this 
component alone can be isolated to further study the concentration. 

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{../fig/PCA_OBPG_2_3.png}
    \caption{Performing PCA with 2 (left) and 3 (right) principal components, 
    then atmospheric correction followed by the NASA OBPG algorithm}
    \label{fig:pca_obpg}
\end{figure}